env:
  personas:
    A:
      triggers:
        # 3人共通（15個）
        - "お金"
        - "外見"
        - "時間や締め切り"
        - "健康"
        - "仕事"
        - "恋愛"
        - "政治"
        - "宗教"
        - "法律・権利"
        - "自然・環境"
        - "家族"
        - "友人"
        - "交通"
        - "天気"
        - "プライバシー・セキュリティ"
        # AB共通（5個）
        - "勉強"
        - "研究"
        - "読書"
        - "歴史"
        - "宇宙"
        # CA共通（5個）
        - "旅行"
        - "温泉"
        - "食べ物"
        - "映画"
        - "国際・異文化"
    B:
      triggers:
        # 3人共通（15個）
        - "お金"
        - "外見"
        - "時間や締め切り"
        - "健康"
        - "仕事"
        - "恋愛"
        - "政治"
        - "宗教"
        - "法律・権利"
        - "自然・環境"
        - "家族"
        - "友人"
        - "交通"
        - "天気"
        - "プライバシー・セキュリティ"
        # AB共通（5個）
        - "勉強"
        - "研究"
        - "読書"
        - "歴史"
        - "宇宙"
        # BC共通（5個）
        - "ゲーム"
        - "アニメ"
        - "ドラマ"
        - "音楽"
        - "動物・ペット"
    C:
      triggers:
        # 3人共通（15個）
        - "お金"
        - "外見"
        - "時間や締め切り"
        - "健康"
        - "仕事"
        - "恋愛"
        - "政治"
        - "宗教"
        - "法律・権利"
        - "自然・環境"
        - "家族"
        - "友人"
        - "交通"
        - "天気"
        - "プライバシー・セキュリティ"
        # BC共通（5個）
        - "ゲーム"
        - "アニメ"
        - "ドラマ"
        - "音楽"
        - "動物・ペット"
        # CA共通（5個）
        - "旅行"
        - "温泉"
        - "食べ物"
        - "映画"
        - "国際・異文化"
  max_steps: 8  # ステップ数ベースの終了条件
  max_rounds: 6  # ラウンド数ベースの終了条件（1ラウンド = 3人間発話、6ラウンド = 18発話）。今は使ってない
  include_robot: true
  intervention_max_history: 3  # 介入判定LLMへの会話履歴（人間発話数）
  robot_max_history: 6  # ロボット発言LLMへの会話履歴（人間発話数）
  max_history_human: 12  # 人間LLMへの会話履歴（人間発話数）
  max_history_relation: 3  # 関係性LLMへの会話履歴（人間発話数）
  reward_backend: "context_llm"
  debug: true
  start_relation_check_after_utterances: 3  # 何発話後から関係性推定を開始するか
  evaluation_horizon: 3  # ロボットの何発話後の関係性を報酬に使うか
  max_auto_skip: 10  # 安定状態が何回続いたら話題転換するか

  # === 報酬パラメータ（新構造：感情ニーズベース + 2段階ボーナス） ===
  horizon_bonus: 1.0  # evaluation_horizon後に安定+ターゲットエッジ+正解戦略で付与
  terminal_bonus: 1.0  # terminal_bonus_duration後も安定+ターゲットエッジ+正解戦略で付与（追加）
  preference_match_bonus: 0.5  # 人間の好み（感情ニーズ）に一致した戦略選択時のボーナス
  terminal_bonus_duration: 3  # horizon_bonus後、さらに安定が続く必要がある発話数

  # === 旧報酬パラメータ（互換性のため維持） ===
  stable_bonus: 2.0  # 旧パラメータ（horizon_bonus + terminal_bonusに置き換え）
  time_penalty: 0.1  # 毎ステップのペナルティ（現在は未使用）
  intervention_cost: 0.35  # ロボット介入時のコスト（現在は未使用）
  output_error_penalty: 1.0  # LLM出力失敗時のペナルティ
  min_robot_intervention_lookback: 6  # 安定ボーナスのための介入確認範囲（現在は未使用）

scorer:
  backend: "azure"
  use_ema: false
  decay_factor: 1.5

topic_manager:
  enable: true  # トピック提案機能を有効化するか
  generation_prompt: |
    あなたは人間3人が話すための、話題を1つだけ提案するアシスタントです。
    [既に提案した話題]は提示しないでください。

    以下のカテゴリーに関連する話題を生成してください：
    {trigger_examples}

    このカテゴリーに直接的に関連する具体的な話題を提案してください。
    例：
    - 「お金」カテゴリー → 「最近の物価高について」「投資の始め方」
    - 「ゲーム」カテゴリー → 「最新のゲームトレンド」「eスポーツの将来」
    - 「映画」カテゴリー → 「今年のアカデミー賞作品」「好きな映画監督」

    話題を1フレーズ（3-10語）で1つだけ提案してください。

ollama:
  # ollama API URL
  bases:
    - "http://ollama_a:11434"  # GPU4 (persona A)
    - "http://ollama_b:11435"  # GPU4 (persona B)
    - "http://ollama_c:11436"  # GPU5 (persona C)
  model: "gpt-oss:20b"
  model_rl: "gpt-oss:20b"
  scorer_base: "http://ollama_d:11434"  # GPU5 (relation scorer)
  reward_base: "http://ollama_reward:11434"  # GPU4 (reward evaluator)
  gen_options:
    temperature: 0.75
    top_p: 0.9
    num_ctx: 2048

wandb:
  enabled: true  # wandb可視化を有効化
  # 以下の設定は .env ファイルで設定してください
  # project: 環境変数 WANDB_PROJECT で設定（デフォルト: "rl-convo-policy"）
  # entity: 環境変数 WANDB_ENTITY で設定
  # table_log_frequency: 自動計算（バッチサイズと同じ = per_device_train_batch_size × grad_accum_steps × 2）

llm:
  provider: "azure"
  # 以下の機密情報は .env ファイルで設定してください
  # azure_endpoint: 環境変数 AZURE_ENDPOINT で設定
  # azure_api_key: 環境変数 AZURE_API_KEY で設定
  # azure_api_version: 環境変数 AZURE_API_VERSION で設定（デフォルト: "2024-12-01-preview"）
  # azure_model: 環境変数 AZURE_MODEL で設定（デフォルト: "gpt-5-chat"）
  # azure_embedding_deployment: 環境変数 AZURE_EMBEDDING_DEPLOYMENT で設定（デフォルト: "text-embedding-3-small"）
  # azure_embedding_api_version: 環境変数 AZURE_EMBEDDING_API_VERSION で設定（デフォルト: "2024-12-01-preview"）
  # reasoning_effort: 環境変数 REASONING_EFFORT で設定（デフォルト: "minimal"）
  # enable_reasoning_param: 環境変数 ENABLE_REASONING_PARAM で設定（デフォルト: false）
  max_attempts: 5
  base_backoff: 0.5

  # 各LLM用の個別モデル設定
  # 以下も .env ファイルで設定可能です
  # human_model: 環境変数 HUMAN_MODEL で設定（デフォルト: "gpt-4.1"）
  # relation_model: 環境変数 RELATION_MODEL で設定（デフォルト: "gpt-4.1"）
  # robot_model: 環境変数 ROBOT_MODEL で設定（デフォルト: "gpt-5-chat"）
  # topic_model: 環境変数 TOPIC_MODEL で設定（デフォルト: "gpt-5-chat"）
  # intervention_model: 環境変数 INTERVENTION_MODEL で設定（デフォルト: "gpt-5-chat"）

ppo:
  model_name_or_path: "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5"  # 日本語強化Llama-3.1モデル
  # 他の選択肢:
  # - unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit　日本語対応してない
  # - unsloth/Qwen3-8B-unsloth-bnb-4bit
  # - unsloth/Qwen3-14B-unsloth-bnb-4bit
  # - unsloth/gpt-oss-20b-unsloth-bnb-4bit
  # - unsloth/Qwen2.5-14B-Instruct-bnb-4bit
  revision: null  # 特定のコミットやブランチを指定するためのオプション
  lora_rank: 16  # LoRAのランク
  
  # === 学習率 ===
  lr: 1.0e-4  # 学習率を上げて推定精度向上を加速（5e-5 → 1e-4）
  lr_scheduler_type: "cosine"  # 学習率スケジューラー ("constant", "linear", "cosine", "polynomial")
  warmup_ratio: 0.01  # ウォームアップ期間（総ステップ数の1%）
  # min_lr_ratio: 0.25  # 終了時の学習率（cosineスケジューラーのデフォルトは0）
  
  # === バッチサイズ ===
  per_device_train_batch_size: 4  # 1デバイスあたりのバッチサイズ
  per_device_eval_batch_size: 4  # 評価時の1デバイスあたりのバッチサイズ
  ppo_epochs: 2  # ppoのエポック数（過学習防止のため4→3に削減）
  grad_accum_steps: 4  # 勾配累積ステップ数（実効バッチサイズ = per_device_train_batch_size × grad_accum_steps × 2)）
  num_mini_batches: 2  # ミニバッチ数を増やして更新を細かく（1 → 4、安定化）

  # === LLMの多様性 ===
  max_new_tokens: 2  # 生成する最大トークン数
  temperature: 0.6
  top_p: 0.9  # Qwen3では0.8が推奨
  decode_typical_p: 0.90  # typical samplingも調整（0.95 → 0.90）
  
  # === エントロピー正則化（学習中の探索促進） ===
  # 【戦略】temperature低 + フェーズ別自動調整
  # - 学習初期: 高い目標エントロピー → 多様な戦略を試す（探索）
  # - 学習中期: 中程度の目標 → 有望な戦略を深掘り（バランス）
  # - 学習後期: 低い目標 → 確信度の高い選択に収束（活用）
  # - 推論時: 低temperature → 学習済みの確信をそのまま使う
  # → 結果: 「状況に応じて適切に多様」だが「同じ状況では確信的」

  # === フェーズ別自動調整方式（推奨） ===
  entropy_auto_adjust: true  # 自動調整を有効化
  entropy_coef_initial: 0.1  # エントロピー係数の初期値
  entropy_coef_min: 0.01  # 係数の最小値（低すぎる探索を防ぐ）
  entropy_coef_max: 0.3  # 係数の最大値（過剰探索を防ぐ）
  entropy_adjust_beta: 0.3  # 調整の強さ（0.05 → 0.1: より積極的に調整）
  
  # フェーズ切り替えタイミング
  entropy_transition_updates: [0.3, 0.6]  # 30%, 60%でフェーズ切り替え
  
  # フェーズ1（初期0-30%）: 高い探索
  entropy_target_phase1: 0.7  # 高い目標 → 全戦略を均等に試す
  # H_observed < 0.7 なら係数を上げて探索を促進
  # H_observed > 0.7 なら係数を下げて過剰探索を抑制
  
  # フェーズ2（中期30-60%）: バランス
  entropy_target_phase2: 0.4  # 中程度の目標 → 有望な戦略に注目
  # 探索と活用のバランスを取る
  
  # フェーズ3（後期60-100%）: 収束
  entropy_target_phase3: 0.2  # 低い目標 → 確信的な選択に収束
  # 学習した最適戦略に集中
  
  max_entropy_bonus: 0.5  # エントロピーボーナスの上限クリップ

  # === 従来のフェーズ方式（entropy_auto_adjust=falseの場合に使用） ===
  # 固定係数モード（自動調整なし）
  entropy_coef_phase1: 0.05  # フェーズ1の固定係数
  entropy_coef_phase2: 0.03  # フェーズ2の固定係数
  entropy_coef_phase3: 0.01  # フェーズ3の固定係数
  
  # === 単一目標方式（後方互換性のため維持） ===
  entropy_target: 0.7  # 全フェーズ共通の目標（phase別設定がない場合のデフォルト）


  # === KL制約の緩和（探索促進） ===
  kl_coef: 0.05  # KL係数の初期値 大きくすると探索が抑制される
  min_kl_coef: 0.01  # KL係数の下限
  max_kl_coef: 0.3  # KL係数の上限
  kl_estimator: "k3"  # KL推定器を"k3"に変更（分散が低い推定器、KL安定化）
  target_kl: 0.05  # 一度の更新の目標変化量
  
  # === 自動KL調整パラメータ ===
  kl_adjust_up: 1.5  # KL > 1.5×target時にkl_coefを乗算する係数
  kl_adjust_down: 0.85  # KL < 0.5×target時にkl_coefを乗算する係数
  kl_lr_adjust_up: 0.9  # KL急上昇時のLR調整係数（LR *= 0.9）
  kl_lr_adjust_down: 1.1  # KL低下時のLR調整係数（LR *= 1.1、上限=base_lr）
  

  # === クリッピングと勾配制御 ===
  cliprange: 0.3  # 元のモデルから0.7-1.3までを許容するということ。これを超えた割合がclipfrac_avg
  cliprange_value: 0.2  # 価値関数のクリッピングも調整（0.2 → 0.15）
  max_grad_norm: 0.5  # 勾配クリッピングを緩和（0.3 → 0.5、学習を促進）

  # === 報酬処理 ===
  filter_zero_rewards: true  # 安定状態を自動スキップ（不安定になるまで人間発話を生成）
  whiten_rewards: true  # 報酬のホワイトニングを有効化（true → false、報酬の分散を保持）
  # local_mini_batch_size = per_device_train_batch_size × grad_accum_steps / num_processes >= 8 でないと使えない

  # === 価値関数とアドバンテージ ===
  vf_coef: 0.1  # 価値関数損失の係数を下げる（0.1 → 0.05、過学習防止）
  gamma: 0.8  # 割引率（1.0=割引なし、エピソード内の全報酬を等しく評価）
  lam: 0.95  # GAE lambda（0.95が標準、高いほど長期的報酬を重視）

  # === エントロピー監視 ===
  entropy_floor: 0.03  # エントロピー閾値を大幅に上げる（0.02 → 0.5、探索を強く促進）
  entropy_patience: 20  # エントロピーが閾値を下回る連続回数を増やす（10 → 20、早期停止を抑制）
  entropy_monitor_warmup: 2  # ウォームアップ期間を短縮（20 → 5、早期に監視開始）
  
  total_updates: 15  # 総更新回数（学習時間と性能のバランスを調整）
  output_dir: "../models/ppo_robot"  # ppoモデルの保存先ディレクトリ
  enable_deterministic_eval: false  # 決定的評価を有効化
  deterministic_eval_frequency: 3  # 何ステップごとに評価するか
  max_memory_per_device_gib: 44  # GPUごとの最大使用量 (GiB, 49GiB GPUから余裕を持って使用)
  cuda_visible_devices: "0,1,2,3,4,5"  # 全GPU（0-5）を使用
  device_map_strategy: "balanced_low_0"  # デバイスマップ戦略
  ref_device: 5  # 参照モデルは単一GPU（GPU 5）に配置
  prompt_feed_debug: false  # 学習時のプロンプト全文ダンプを抑止
  local_rollout_forward_batch_size: 1
  gradient_checkpointing: true
