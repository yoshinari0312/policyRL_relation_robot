from __future__ import annotations

import asyncio
import copy
import json
import math
from typing import Any, Dict, Iterable, List, Optional, Tuple

from azure_clients import get_azure_chat_completion_client, build_chat_completion_params
from config import get_config
from context_reward import score_context_alignment
from humans_ollama import human_reply
from network_metrics import BALANCED, UNBALANCED, analyze_relations_from_scores
from relation_scorer import RelationScorer
from utils import filter_logs_by_human_count

_PLANNER_STRATEGIES = ("reframe", "validate", "bridge")

# ãƒ—ãƒ©ãƒ³æ¤œè¨¼ç”¨ã®å®šæ•°
_TARGET_EDGES = {"AB", "BC", "CA"}  # å¤‰æ›´å¯¾è±¡ã¨ãªã‚‹ã‚¨ãƒƒã‚¸ï¼ˆãƒšã‚¢é–¢ä¿‚ï¼‰
_ALLOWED_STRATEGIES = _PLANNER_STRATEGIES  # è¨±å¯ã•ã‚Œã‚‹ä»‹å…¥æˆ¦ç•¥
_TARGET_SPEAKERS = {"A", "B", "C"}  # ä»‹å…¥å¯¾è±¡ã¨ãªã‚‹è©±è€…

_STABLE_SIGN_PATTERNS: Dict[str, Dict[str, int]] = {
    "+++": {"AB": +1, "BC": +1, "CA": +1},
    "+--": {"AB": +1, "BC": -1, "CA": -1},
    "-+-": {"AB": -1, "BC": +1, "CA": -1},
    "--+": {"AB": -1, "BC": -1, "CA": +1},
}


def _strip_quotes(text: str) -> str:
    # å…ˆé ­ãƒ»æœ«å°¾ã®å¼•ç”¨ç¬¦ã‚„æ‹¬å¼§ã‚’å‰Šã‚‹
    return text.strip().strip("\"'\u300c\u300d\u300e\u300f()[]ï¼ˆï¼‰ã€ã€‘")


def _coerce_bool(value: Any) -> Optional[bool]:
    # Boolean ã¸ã®å¤‰æ›ã‚’è©¦ã¿ã‚‹
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        lowered = value.strip().lower()
        if lowered in {"true", "1", "yes"}:
            return True
        if lowered in {"false", "0", "no"}:
            return False
    return None


def _coerce_int(value: Any) -> Optional[int]:
    # Integer ã¸ã®å¤‰æ›ã‚’è©¦ã¿ã‚‹
    try:
        return int(value)
    except (TypeError, ValueError):
        return None


class ConversationEnv:
    """ ä¸‰è€…ä¼šè©±ç’°å¢ƒ """

    def __init__(
        self,
        *,
        max_steps: int,
        personas: Optional[Iterable[str]] = None,
        include_robot: bool = True,
        max_history: int = 12,
        backend: Optional[str] = None,
        decay_factor: float = 1.5,
        debug: bool = False,
        reward_backend: str = "rule",
        evaluation_horizon: int = 2,
        time_penalty: Optional[float] = None,
        terminal_bonus: Optional[float] = None,
        intervention_cost: Optional[float] = None,
    ) -> None:
        cfg = get_config()
        use_ema_cfg = getattr(cfg.scorer, "use_ema", True)
        if use_ema_cfg is None:
            use_ema_cfg = False

        self.max_steps = max(1, int(max_steps))  # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™

        # max_rounds ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œã‚’ä½¿ç”¨ã—ã¦ç›®æ¨™äººé–“ç™ºè©±æ•°ã‚’è¨ˆç®—
        # max_rounds ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€max_steps ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨
        max_rounds_cfg = getattr(cfg.env, "max_rounds", None)
        if max_rounds_cfg is not None:
            self.max_rounds = max(1, int(max_rounds_cfg))
            # 1ãƒ©ã‚¦ãƒ³ãƒ‰ = len(persona_pool) äººé–“ç™ºè©±ï¼ˆé€šå¸¸3ç™ºè©±ï¼‰
            # persona_pool ã¯ã¾ã åˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ã‚’ä½¿ç”¨
            self.target_human_utterances = self.max_rounds * 3
        else:
            # max_rounds ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€max_steps ã‹ã‚‰æ¨å®š
            self.max_rounds = self.max_steps
            self.target_human_utterances = self.max_steps * 3

        self.include_robot = bool(include_robot)
        self.max_history = max(1, int(max_history))  # ä»‹å…¥åˆ¤å®šLLMç”¨
        self.max_history_human = max(1, int(getattr(cfg.env, "max_history_human", 12)))  # äººé–“LLMç”¨
        self.max_history_relation = max(1, int(getattr(cfg.env, "max_history_relation", 3)))  # é–¢ä¿‚æ€§LLMç”¨
        self.debug = bool(debug)
        self.reward_backend = (reward_backend or "rule").lower()
        self.evaluation_horizon = max(1, int(evaluation_horizon))
        self.start_relation_check_after_utterances = getattr(cfg.env, "start_relation_check_after_utterances", 3)

        # å ±é…¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’YAMLã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆå¼•æ•°ã§æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
        self.time_penalty = float(time_penalty) if time_penalty is not None else float(getattr(cfg.env, "time_penalty", 0.01))
        self.terminal_bonus = float(terminal_bonus) if terminal_bonus is not None else float(getattr(cfg.env, "terminal_bonus", 0.25))
        self.intervention_cost = float(intervention_cost) if intervention_cost is not None else float(getattr(cfg.env, "intervention_cost", 0.02))

        # æ–°ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’YAMLã‹ã‚‰èª­ã¿è¾¼ã¿
        self.min_robot_intervention_lookback = int(getattr(cfg.env, "min_robot_intervention_lookback", 6))
        self.terminal_bonus_duration = int(getattr(cfg.env, "terminal_bonus_duration", 2))

        # ãƒ­ãƒœãƒƒãƒˆä»‹å…¥å±¥æ­´ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        self.steps_since_last_intervention: int = 0  # æœ€å¾Œã®ä»‹å…¥ã‹ã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
        self.stable_step_start: Optional[int] = None  # å®‰å®šçŠ¶æ…‹ãŒé–‹å§‹ã•ã‚ŒãŸã‚¹ãƒ†ãƒƒãƒ—ï¼ˆNoneãªã‚‰æœªå®‰å®šï¼‰
        self.terminal_bonus_given: bool = False  # terminal_bonusãŒæ—¢ã«ä¸ãˆã‚‰ã‚ŒãŸã‹ã©ã†ã‹

        # personasã®å‡¦ç†ï¼ˆListå½¢å¼ã¨Dictå½¢å¼ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆï¼‰
        if personas is None:
            # personasãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€è¨­å®šã‹ã‚‰èª­ã¿è¾¼ã‚€
            personas_cfg = getattr(cfg.env, "personas", None)
            if personas_cfg and isinstance(personas_cfg, dict):
                self.persona_pool: List[str] = sorted(personas_cfg.keys())
                self.persona_triggers: Dict[str, List[str]] = {
                    name: info.get("triggers", []) if isinstance(info, dict) else []
                    for name, info in personas_cfg.items()
                }
            else:
                self.persona_pool: List[str] = []
                self.persona_triggers: Dict[str, List[str]] = {}
        elif isinstance(personas, dict):
            # Dictå½¢å¼: {"A": {"triggers": [...]}, "B": {...}, ...}
            self.persona_pool = sorted(personas.keys())
            self.persona_triggers = {
                name: info.get("triggers", []) if isinstance(info, dict) else []
                for name, info in personas.items()
            }
        else:
            # Listå½¢å¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰: ["A", "B", "C"]
            persona_list = [p for p in personas if p]
            self.persona_pool = list(persona_list)
            self.persona_triggers = {}

        # persona_pool ãŒåˆæœŸåŒ–ã•ã‚ŒãŸã®ã§ã€æ­£ç¢ºãª target_human_utterances ã‚’è¨ˆç®—
        num_personas = len(self.persona_pool) if self.persona_pool else 3
        self.target_human_utterances = self.max_rounds * num_personas

        self._scorer_kwargs = {
            "backend": backend,
            "use_ema": use_ema_cfg,
            "decay_factor": float(decay_factor),
            "verbose": bool(debug),
        }
        self.scorer = RelationScorer(**self._scorer_kwargs)

        self.logs: List[Dict[str, Any]] = []
        self.episode = 0
        self._episode_step = 0
        self.t = 0
        self.total_steps = 0
        self.used_topics: List[str] = []  # æ—¢ã«ä½¿ç”¨ã—ãŸãƒˆãƒ”ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆ
        self.current_topic: Optional[str] = None  # ç¾åœ¨ã®ãƒˆãƒ”ãƒƒã‚¯
        self.current_topic_trigger: Optional[str] = None  # ç¾åœ¨ã®ãƒˆãƒ”ãƒƒã‚¯ã®å…ƒã«ãªã£ãŸåœ°é›·

        self._last_observation: Optional[str] = None
        self.reset()

    def reset(self) -> str:
        # ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦åˆæœŸçŠ¶æ…‹ã‚’è¿”ã™
        self.episode += 1
        self._episode_step = 0
        self.t = 0
        self.logs = []
        self.scorer = RelationScorer(**self._scorer_kwargs)

        # ãƒ­ãƒœãƒƒãƒˆä»‹å…¥å±¥æ­´ã®ãƒªã‚»ãƒƒãƒˆ
        self.steps_since_last_intervention = 0
        self.stable_step_start = None
        self.terminal_bonus_given = False

        # ãƒˆãƒ”ãƒƒã‚¯ã‚’ç”Ÿæˆ
        self.current_topic = self._get_topic_suggestion()
        self.used_topics.append(self.current_topic)

        # åˆæœŸä¼šè©±ã‚’ç”Ÿæˆï¼ˆevaluation_horizonå›ã®ã‚¿ãƒ¼ãƒ³ = é€šå¸¸1ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
        self._bootstrap_humans(self.evaluation_horizon)

        # _ensure_unstable_seed()ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        # åˆæœŸçŠ¶æ…‹ãŒå®‰å®šã®å ´åˆã§ã‚‚è¿½åŠ ç™ºè©±ã‚’ç”Ÿæˆã—ãªã„ã‚ˆã†ã«ã™ã‚‹
        # è¨“ç·´æ™‚ã«ä¸å®‰å®šãªåˆæœŸçŠ¶æ…‹ãŒå¿…è¦ãªå ´åˆã¯ã€ã“ã®è¡Œã‚’æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„
        # self._ensure_unstable_seed()

        observation = self._make_observation()
        self._last_observation = observation
        return observation

    def step(self, action: str) -> Tuple[str, float, bool, Dict[str, Any]]:
        """
        ç’°å¢ƒã«1ã‚¹ãƒ†ãƒƒãƒ—ä½œç”¨ã‚’ä¸ãˆã€(æ¬¡çŠ¶æ…‹, å ±é…¬, çµ‚äº†ãƒ•ãƒ©ã‚°, æƒ…å ±è¾æ›¸) ã‚’è¿”ã™

        å³åº§å ±é…¬ã®è¨­è¨ˆï¼ˆIMMEDIATE_REWARD_DESIGN.mdã‚’å‚ç…§ï¼‰:
        1. äººé–“ç™ºè©±ã‚’1å›ç”Ÿæˆ
        2. é–¢ä¿‚æ€§ã‚’è©•ä¾¡
        3. å®‰å®šãªã‚‰å ±é…¬ãªã—ã§çµ‚äº†
        4. ä¸å®‰å®šãªã‚‰ä»‹å…¥åˆ¤å®š
        5. ä»‹å…¥ã™ã‚‹å ´åˆ:
           - evaluation_horizonå›ã®äººé–“ç™ºè©±ã‚’å†…éƒ¨ç”Ÿæˆï¼ˆä¸¦è¡Œå‡¦ç†ï¼‰
           - åå®Ÿä»®æƒ³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚‚ä¸¦è¡Œå®Ÿè¡Œ
           - å®‰å®šã«ãªã£ãŸã‚‰terminal_bonus_durationå›è¿½åŠ 
        6. å ±é…¬ã‚’è¨ˆç®—ã—ã¦å³åº§ã«è¿”ã™

        ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€PPOãŒå­¦ç¿’å¯èƒ½ãªå³åº§å ±é…¬ã‚’å®Ÿç¾
        """
        if self.debug:
            print(f"\nğŸ”§ [DEBUG] step() é–‹å§‹")
            print(f"  ç¾åœ¨ã®ãƒ­ã‚°æ•°: {len(self.logs)}")
            print(f"  _episode_step: {self._episode_step}")
            print(f"  steps_since_last_intervention: {self.steps_since_last_intervention}")

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜ï¼ˆäººé–“ç™ºè©±ç”Ÿæˆå‰ï¼‰
        snapshot_logs = [dict(entry) for entry in self.logs]

        # äººé–“ç™ºè©±ã‚’1å›ç”Ÿæˆï¼ˆæ¡ä»¶ä»˜ãï¼‰
        # - ã‚¹ãƒ†ãƒƒãƒ—1ï¼ˆ_episode_step == 0ï¼‰: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆreset()ã§æ—¢ã«ç”Ÿæˆæ¸ˆã¿ï¼‰
        # - å‰ã‚¹ãƒ†ãƒƒãƒ—ã§ä»‹å…¥ã—ãªã‹ã£ãŸï¼ˆsteps_since_last_intervention > 0ï¼‰: 1ç™ºè©±ç”Ÿæˆ
        # - å‰ã‚¹ãƒ†ãƒƒãƒ—ã§ä»‹å…¥ã—ãŸï¼ˆsteps_since_last_intervention == 0ï¼‰: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆevaluation_horizonåˆ†ãŒæ—¢ã«ç”Ÿæˆæ¸ˆã¿ï¼‰
        should_generate_human = self._episode_step > 0 and self.steps_since_last_intervention > 0

        if should_generate_human:
            if self.debug:
                print(f"  ğŸ‘¤ äººé–“ç™ºè©±ã‚’1å›ç”Ÿæˆä¸­ï¼ˆä¼šè©±ã‚’é€²ã‚ã‚‹ï¼‰...")

            # 1äººé–“ç™ºè©±ã ã‘ã‚’ç”Ÿæˆ
            replies = human_reply(self.logs, self.persona_pool, topic=self.current_topic, topic_trigger=self.current_topic_trigger, num_speakers=1)
            if replies:
                # ç”Ÿæˆã•ã‚ŒãŸ1ç™ºè©±ã‚’è¿½åŠ 
                self.logs.extend(replies)

                # é–¢ä¿‚æ€§ã‚’æ›´æ–°
                participants = self._participants(self.logs)
                if participants:
                    human_utterance_count = sum(1 for log in self.logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')
                    if human_utterance_count >= self.start_relation_check_after_utterances:
                        filtered_logs = filter_logs_by_human_count(self.logs, self.max_history_relation, exclude_robot=True)
                        self.scorer.get_scores(filtered_logs, participants, return_trace=False, update_state=True)

                if self.debug:
                    print(f"  ç”Ÿæˆå¾Œã®ãƒ­ã‚°æ•°: {len(self.logs)}")
                    last_entry = self.logs[-1]
                    print(f"  æœ€æ–°ç™ºè©±: [{last_entry.get('speaker')}] {last_entry.get('utterance', '')[:50]}...")
        else:
            if self.debug:
                reason = "ã‚¹ãƒ†ãƒƒãƒ—1ï¼ˆreset()ã§æ—¢ã«ç”Ÿæˆæ¸ˆã¿ï¼‰" if self._episode_step == 0 else "å‰ã‚¹ãƒ†ãƒƒãƒ—ã§ä»‹å…¥æ¸ˆã¿ï¼ˆevaluation_horizonåˆ†ãŒæ—¢ã«ç”Ÿæˆæ¸ˆã¿ï¼‰"
                print(f"  â­ï¸  äººé–“ç™ºè©±ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ{reason}ï¼‰")

        # é–¢ä¿‚æ€§ã‚’è©•ä¾¡ï¼ˆäººé–“ç™ºè©±ç”Ÿæˆå¾Œã®ãƒ­ã‚°ã‚’ä½¿ç”¨ï¼‰
        participants = self._participants(self.logs)
        human_utterance_count = sum(1 for log in self.logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')
        if human_utterance_count >= self.start_relation_check_after_utterances:
            filtered_logs = filter_logs_by_human_count(self.logs, self.max_history_relation, exclude_robot=True)
            rel, trace_scores = self._relation_state(filtered_logs, update_state=True)
        else:
            rel = {}
            trace_scores = []

        metrics, trace_metrics = self._metrics_state(rel, participants)
        is_stable = metrics.get("unstable_triads", 0) == 0 and bool(self.logs)

        if self.debug:
            print(f"  ğŸ“Š é–¢ä¿‚æ€§è©•ä¾¡:")
            print(f"    ä¸å®‰å®šãƒˆãƒ©ã‚¤ã‚¢ãƒ‰æ•°: {metrics.get('unstable_triads', 0)}")
            print(f"    å®‰å®šçŠ¶æ…‹: {is_stable}")

        # å®‰å®šãªå ´åˆã¯å ±é…¬ãªã—ã§çµ‚äº†
        if is_stable:
            if self.debug:
                print(f"  âœ… å®‰å®šçŠ¶æ…‹ â†’ æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ï¼ˆå ±é…¬ãªã—ï¼‰")

            # steps_since_last_interventionã‚’æ›´æ–°ï¼ˆä»‹å…¥ã—ãªã‹ã£ãŸã®ã§ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆï¼‰
            self.steps_since_last_intervention += 1

            self._episode_step += 1
            self.t += 1
            self.total_steps += 1

            # çµ‚äº†æ¡ä»¶: äººé–“ç™ºè©±æ•°ãŒç›®æ¨™ã«é”ã—ãŸã‹ç¢ºèª
            human_utterance_count = sum(1 for log in self.logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')
            done = human_utterance_count >= self.target_human_utterances

            next_observation = self._make_observation()
            self._last_observation = next_observation

            info = {
                "plan": None,
                "plan_error": None,
                "intervened": False,
                "balanced": True,
                "robot_utterance": None,
                "replies": [entry for entry in self.logs[len(snapshot_logs):]],
                "rel": metrics,
                "reward_breakdown": {},
                "personas": list(self.persona_pool),
                "next_observation": next_observation,
            }

            return next_observation, 0.0, done, info

        # ä¸å®‰å®šãªå ´åˆã¯ä»‹å…¥åˆ¤å®š
        if self.debug:
            print(f"  âŒ ä¸å®‰å®šçŠ¶æ…‹ â†’ ä»‹å…¥åˆ¤å®šã‚’å®Ÿè¡Œ")

        plan, plan_error, direct_utterance = self._parse_plan(action)

        if self.debug:
            print(f"  ğŸ“‹ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³è§£æ:")
            if plan:
                print(f"    intervene_now: {plan.get('intervene_now', False)}")
                if plan.get('intervene_now'):
                    print(f"    edge_to_change: {plan.get('edge_to_change')}")
                    print(f"    strategy: {plan.get('strategy')}")
                    print(f"    target_speaker: {plan.get('target_speaker')}")
            if plan_error:
                print(f"    âš ï¸ ãƒ—ãƒ©ãƒ³ã‚¨ãƒ©ãƒ¼: {plan_error}")

        # ä»‹å…¥åˆ¤å®š
        robot_entry: Optional[Dict[str, Any]] = None
        intervened = False
        if plan and plan.get("intervene_now"):
            if self.debug:
                print(f"  ğŸ¤– ãƒ­ãƒœãƒƒãƒˆä»‹å…¥ã‚’å®Ÿè¡Œ")
            robot_entry = self._render_intervention(plan, simulate=False)
            intervened = True
            if self.debug:
                print(f"    ç™ºè©±å†…å®¹: {robot_entry.get('utterance', '')[:80]}...")
        elif direct_utterance:
            robot_entry = {"speaker": "ãƒ­ãƒœãƒƒãƒˆ", "utterance": _strip_quotes(direct_utterance)}
            intervened = True
        else:
            if self.debug:
                print(f"  â­ï¸  ä»‹å…¥ã—ãªã„é¸æŠ")

        # å ±é…¬ã®åˆæœŸåŒ–
        reward = 0.0
        reward_breakdown: Dict[str, float] = {}
        actual_u_flip = None
        counterfactual_u_flip = None
        delta = None

        if not intervened:
            # Case A: ä»‹å…¥ã—ãªã„å ´åˆ
            # ã‚¿ã‚¤ãƒ ãƒšãƒŠãƒ«ãƒ†ã‚£ã®ã¿å³åº§ã«ä»˜ä¸
            if self.debug:
                print(f"  ğŸ’¸ ã‚¿ã‚¤ãƒ ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ä»˜ä¸: {-self.time_penalty:.4f}")
            reward = -self.time_penalty
            reward_breakdown["time_penalty"] = -self.time_penalty

            self.steps_since_last_intervention += 1

        else:
            # Case B: ä»‹å…¥ã™ã‚‹å ´åˆ
            # ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’è¿½åŠ 
            self.logs.append(robot_entry)

            if self.debug:
                print(f"  ğŸ”„ ä¸¦è¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
                print(f"    - å®Ÿä¸–ç•Œ: evaluation_horizon={self.evaluation_horizon}å›ã®äººé–“ç™ºè©±")
                print(f"    - åå®Ÿä»®æƒ³: ä»‹å…¥ã—ãªã‹ã£ãŸå ´åˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")

            # ä¸¦è¡Œå‡¦ç†: å®Ÿä¸–ç•Œã¨åå®Ÿä»®æƒ³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åŒæ™‚å®Ÿè¡Œ
            async def run_parallel_simulation():
                # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆãƒ­ãƒœãƒƒãƒˆç™ºè©±å‰ï¼‰
                snapshot = [dict(entry) for entry in snapshot_logs]

                # å®Ÿä¸–ç•Œ: ãƒ­ãƒœãƒƒãƒˆç™ºè©±å¾Œã€evaluation_horizonå›ã®äººé–“ç™ºè©±
                # ï¼ˆself.logsã‚’ç›´æ¥ä½¿ç”¨ã›ãšã€æ–°ã—ã„ã‚¹ã‚³ã‚¢ãƒ©ãƒ¼ã§ç®¡ç†ï¼‰
                actual_scorer = RelationScorer(**self._scorer_kwargs)
                actual_logs_task = self._bootstrap_humans_async(
                    self.logs.copy(),
                    self.evaluation_horizon,
                    actual_scorer
                )

                # åå®Ÿä»®æƒ³: ä»‹å…¥ã—ãªã‹ã£ãŸå ´åˆï¼ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ï¼‰
                counterfactual_task = self._simulate_forward_async(
                    snapshot,
                    self.evaluation_horizon,
                    plan=None
                )

                # ä¸¡æ–¹ã®å®Œäº†ã‚’å¾…ã¤
                actual_logs, (cf_u_flip, _) = await asyncio.gather(
                    actual_logs_task,
                    counterfactual_task
                )

                return actual_logs, cf_u_flip

            # asyncio.run()ã§åŒæœŸçš„ã«å®Ÿè¡Œ
            actual_logs, counterfactual_u_flip = asyncio.run(run_parallel_simulation())

            if self.debug:
                print(f"  âœ… ä¸¦è¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
                print(f"    åå®Ÿä»®æƒ³ä¸å®‰å®šåº¦: {counterfactual_u_flip:.4f}")

            # å®Ÿä¸–ç•Œã®ãƒ­ã‚°ã‚’ç’°å¢ƒã«åæ˜ 
            self.logs = actual_logs

            if self.debug:
                print(f"  ğŸ“ å®Ÿä¸–ç•Œã®ä¼šè©±ãƒ­ã‚°ã‚’æ›´æ–°ï¼ˆ{len(self.logs)}ç™ºè©±ï¼‰")

            # evaluation_horizonå¾Œã®é–¢ä¿‚æ€§ã‚’è©•ä¾¡
            participants_after = self._participants(self.logs)
            human_utterance_count_after = sum(1 for log in self.logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')

            if human_utterance_count_after >= self.start_relation_check_after_utterances:
                filtered_logs_after = filter_logs_by_human_count(self.logs, self.max_history_relation, exclude_robot=True)
                rel_after, _ = self._relation_state(filtered_logs_after, update_state=True)
                actual_u_flip = self._compute_u_flip_from_scores(rel_after)
            else:
                rel_after = {}
                actual_u_flip = 1.0

            metrics_after, _ = self._metrics_state(rel_after, participants_after)
            is_stable_after = metrics_after.get("unstable_triads", 0) == 0

            if self.debug:
                print(f"  ğŸ“Š evaluation_horizonå¾Œã®é–¢ä¿‚æ€§:")
                print(f"    å®Ÿéš›ã®ä¸å®‰å®šåº¦: {actual_u_flip:.4f}")
                print(f"    å®‰å®šçŠ¶æ…‹: {is_stable_after}")

            # å ±é…¬è¨ˆç®—
            delta = counterfactual_u_flip - actual_u_flip
            reward = delta - self.intervention_cost - self.time_penalty
            reward_breakdown["delta_u_flip"] = delta
            reward_breakdown["counterfactual_u_flip"] = counterfactual_u_flip
            reward_breakdown["actual_u_flip"] = actual_u_flip
            reward_breakdown["intervention_cost"] = -self.intervention_cost
            reward_breakdown["time_penalty"] = -self.time_penalty

            if self.debug:
                print(f"  ğŸ’° å ±é…¬è¨ˆç®—:")
                print(f"    é–¢ä¿‚æ€§æ”¹å–„åŠ¹æœ: {delta:.4f}")
                print(f"    ä»‹å…¥ã‚³ã‚¹ãƒˆ: {-self.intervention_cost:.4f}")
                print(f"    ã‚¿ã‚¤ãƒ ãƒšãƒŠãƒ«ãƒ†ã‚£: {-self.time_penalty:.4f}")
                print(f"    å°è¨ˆ: {reward:.4f}")

            # å®‰å®šã«ãªã£ãŸå ´åˆã€terminal_bonus_durationãƒã‚§ãƒƒã‚¯
            if is_stable_after:
                if self.debug:
                    print(f"  ğŸ¯ å®‰å®šé”æˆ â†’ terminal_bonusãƒã‚§ãƒƒã‚¯é–‹å§‹")
                    print(f"    è¿½åŠ ã§{self.terminal_bonus_duration}äººé–“ç™ºè©±åˆ†ã®å®‰å®šæ€§ã‚’ç¢ºèª")

                # terminal_bonus_durationäººé–“ç™ºè©±ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯1ãƒ©ã‚¦ãƒ³ãƒ‰ = 3äººé–“ç™ºè©±ãŒæœ€å°å˜ä½ï¼‰
                self._bootstrap_humans(self.terminal_bonus_duration)

                # æœ€å¾Œã®é–¢ä¿‚æ€§ã‚’å†è©•ä¾¡
                participants_check = self._participants(self.logs)
                human_count_check = sum(1 for log in self.logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')

                stability_maintained = True
                if human_count_check >= self.start_relation_check_after_utterances:
                    filtered_check = filter_logs_by_human_count(self.logs, self.max_history_relation, exclude_robot=True)
                    rel_check, _ = self._relation_state(filtered_check, update_state=True)
                    metrics_check, _ = self._metrics_state(rel_check, participants_check)

                    if metrics_check.get("unstable_triads", 0) > 0:
                        # ä¸å®‰å®šã«æˆ»ã£ãŸ
                        stability_maintained = False
                        if self.debug:
                            print(f"    âŒ ä¸å®‰å®šã«æˆ»ã£ãŸ")
                    else:
                        if self.debug:
                            print(f"    âœ… å®‰å®šç¶­æŒ")

                # terminal_bonus_durationäººé–“ç™ºè©±å¾Œã‚‚å®‰å®šãŒç¶šã„ãŸå ´åˆ
                if stability_maintained:
                    if self.debug:
                        print(f"  ğŸ å®‰å®šãŒæŒç¶š â†’ terminal_bonusä»˜ä¸: +{self.terminal_bonus:.4f}")
                    reward += self.terminal_bonus
                    reward_breakdown["terminal_bonus"] = self.terminal_bonus
                elif self.debug:
                    print(f"  âš ï¸  å®‰å®šãŒæŒç¶šã›ãš â†’ terminal_bonusãªã—")

            self.steps_since_last_intervention = 0

        # ã‚¹ãƒ†ãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ã‚’æ›´æ–°
        self._episode_step += 1
        self.t += 1
        self.total_steps += 1

        # æœ€çµ‚çš„ãªé–¢ä¿‚æ€§ã¨observation
        final_participants = self._participants(self.logs)
        final_human_count = sum(1 for log in self.logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')

        # çµ‚äº†æ¡ä»¶: äººé–“ç™ºè©±æ•°ãŒç›®æ¨™ã«é”ã—ãŸã‹ç¢ºèª
        done = final_human_count >= self.target_human_utterances

        if final_human_count >= self.start_relation_check_after_utterances:
            final_filtered = filter_logs_by_human_count(self.logs, self.max_history_relation, exclude_robot=True)
            final_rel, _ = self._relation_state(final_filtered, update_state=True)
        else:
            final_rel = {}

        final_metrics, _ = self._metrics_state(final_rel, final_participants)
        final_balanced = final_metrics.get("unstable_triads", 0) == 0

        next_observation = self._make_observation()
        self._last_observation = next_observation

        if self.debug:
            print(f"  ğŸ step()å®Œäº†")
            print(f"    æœ€çµ‚å ±é…¬: {reward:.4f}")
            print(f"    done: {done}")
            print(f"    ç·ãƒ­ã‚°æ•°: {len(self.logs)}")

        # infoè¾æ›¸
        info: Dict[str, Any] = {
            "plan": plan,
            "plan_error": plan_error,
            "intervened": intervened,
            "balanced": final_balanced,
            "robot_utterance": robot_entry["utterance"] if robot_entry else None,
            "replies": [entry for entry in self.logs[len(snapshot_logs):]],
            "rel": final_metrics,
            "reward_breakdown": reward_breakdown,
            "counterfactual_u_flip": counterfactual_u_flip,
            "actual_u_flip": actual_u_flip,
            "delta_u_flip": delta,
            "personas": list(self.persona_pool),
            "next_observation": next_observation,
        }

        # ä»‹å…¥ã—ãŸå ´åˆã€evaluation_horizonå¾Œã®é–¢ä¿‚æ€§ã‚’è¿½åŠ 
        if intervened and 'metrics_after' in locals():
            info["rel_after_horizon"] = metrics_after
            info["stable_after_horizon"] = is_stable_after

        return next_observation, reward, done, info

    def _get_topic_suggestion(self) -> str:
        """LLMã§è©±é¡Œã‚’ç”Ÿæˆã—ã€çŸ­ã„ãƒˆãƒ”ãƒƒã‚¯æ–‡å­—åˆ—ã‚’è¿”ã™ã€‚"""
        import random
        cfg = get_config()
        topic_cfg = getattr(cfg, "topic_manager", None)
        if not topic_cfg or not getattr(topic_cfg, "enable", False):
            # ãƒˆãƒ”ãƒƒã‚¯æ©Ÿèƒ½ãŒç„¡åŠ¹ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒˆãƒ”ãƒƒã‚¯ã‚’è¿”ã™
            self.current_topic_trigger = None
            return "è‡ªç”±ãªè©±é¡Œ"

        system = getattr(topic_cfg, "generation_prompt", "")

        # persona_triggersã‹ã‚‰ trigger_examples ã‚’ç”Ÿæˆ
        # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤ã ã‘é¸æŠ
        selected_trigger = None
        if self.persona_triggers and "{trigger_examples}" in system:
            # å…¨personaã®triggersã‚’å¹³å¦åŒ–ï¼ˆé‡è¤‡ã‚’é™¤ãï¼‰
            all_triggers = []
            for triggers in self.persona_triggers.values():
                all_triggers.extend(triggers)
            # é‡è¤‡ã‚’é™¤å»
            all_triggers = list(set(all_triggers))

            if all_triggers:
                # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤ã ã‘é¸æŠ
                selected_trigger = random.choice(all_triggers)
                trigger_examples = selected_trigger
                system = system.replace("{trigger_examples}", trigger_examples)
            else:
                # triggersãŒç©ºã®å ´åˆã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å‰Šé™¤
                system = system.replace("{trigger_examples}", "æ§˜ã€…ãªãƒ†ãƒ¼ãƒ")

        # é¸æŠã•ã‚ŒãŸåœ°é›·ã‚’ä¿å­˜ï¼ˆäººé–“LLMã§ä½¿ç”¨ã™ã‚‹ãŸã‚ï¼‰
        self.current_topic_trigger = selected_trigger

        used_str = "\n".join(f"- {t}" for t in self.used_topics) if self.used_topics else "(ãªã—)"
        prompt = f"[æ—¢ã«ææ¡ˆã—ãŸè©±é¡Œ]\n{used_str}"

        llm_cfg = getattr(cfg, "llm", None)
        client, deployment = get_azure_chat_completion_client(llm_cfg, model_type="topic")
        max_attempts = getattr(cfg.llm, "max_attempts", 5) or 5
        base_backoff = getattr(cfg.llm, "base_backoff", 0.5) or 0.5
        
        if client and deployment:
            messages = [{"role":"system","content":system},
                        {"role":"user","content":prompt}]
            for attempt in range(1, max_attempts + 1):
                try:
                    # GPT-5ã®å ´åˆã¯reasoningãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    params = build_chat_completion_params(deployment, messages, cfg.llm, temperature=1.0)
                    res = client.chat.completions.create(**params)
                    if res and getattr(res, "choices", None):
                        choice = res.choices[0]
                        message = getattr(choice, "message", None)
                        if isinstance(message, dict):
                            txt = message.get("content", "")
                        else:
                            txt = getattr(message, "content", "")
                        txt = (txt or "").strip()
                        if txt:
                            return txt
                except Exception as exc:
                    if self.debug:
                        print(f"[topic] attempt {attempt} failed:", exc)
                    if attempt < max_attempts:
                        import time
                        time.sleep(base_backoff * (2 ** (attempt - 1)))
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return "æœ€è¿‘ã®å‡ºæ¥äº‹ã«ã¤ã„ã¦"

    def planning_context(self) -> Dict[str, Any]:
        # é–¢ä¿‚å®‰å®šåŒ–ã®ãŸã‚ã®ä»‹å…¥è¨ˆç”»ã‚’ç«‹æ¡ˆã™ã‚‹
        weights = self._edge_weights(self.logs)
        u_flip, stable_sign, distances = self._compute_u_flip(weights)
        target_edge = "AB"
        if distances:
            target_edge = max(distances.items(), key=lambda kv: kv[1])[0]
        return {
            "scores": weights,
            "u_flip": u_flip,
            "stable_sign": stable_sign,
            "distances": distances,
            "target_edge": target_edge,
            "evaluation_horizon": self.evaluation_horizon,
            "time_penalty": self.time_penalty,
            "intervention_cost": self.intervention_cost,
            "balanced": self._is_balanced(),
        }

    def _make_observation(self) -> str:
        context = self.planning_context()
        # max_historyå€‹ã®äººé–“ç™ºè©± + ãã®é–“ã®ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’å–å¾—
        filtered_logs = filter_logs_by_human_count(self.logs, self.max_history)
        history_lines = [f"[{item.get('speaker', '?')}] {item.get('utterance', '').strip()}" for item in filtered_logs]
        if not history_lines:
            history_lines = ["(å±¥æ­´ãªã—)"]

        scores = context.get("scores", {}) or {"AB": 0.0, "BC": 0.0, "CA": 0.0}
        stable_sign = context.get("stable_sign", "???")
        target_edge = context.get("target_edge", "AB")
        eval_horizon = context.get("evaluation_horizon", self.evaluation_horizon)
        time_penalty = context.get("time_penalty", self.time_penalty)
        intervention_cost = context.get("intervention_cost", self.intervention_cost)

        prompt_lines = [
            "ä¸‰è€…ä¼šè©±ï¼ˆè©±è€… A/B/Cï¼‰ã®é–¢ä¿‚ã‚’å®‰å®šåŒ–ã™ã‚‹ãŸã‚ã€ãƒ­ãƒœãƒƒãƒˆãŒé©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ä¸€è¨€ä»‹å…¥ã—ã¾ã™ã€‚",
            "ã‚ãªãŸã®å½¹å‰²ã¯ã€ç¾åœ¨ã®ä¼šè©±å±¥æ­´ã¨å„ãƒšã‚¢ã®é–¢ä¿‚ã‚¹ã‚³ã‚¢ï¼ˆ-1..1ï¼‰ã‚’å—ã‘å–ã‚Šã€"
            "ã€Œã§ãã‚‹ã ã‘æ—©ãé–¢ä¿‚æ€§ã‚’å®‰å®šçŠ¶æ…‹ï¼ˆ+++,+--,-+-,--+ï¼‰ã«ã™ã‚‹ã€ãŸã‚ã®ä»‹å…¥æ–¹æ³•ï¼ˆä»Šä»‹å…¥ã™ã¹ãã‹ãƒ»å¯¾è±¡ãƒªãƒ³ã‚¯ãƒ»ç™ºè©±å…ˆãƒ»ä»‹å…¥ã‚¿ã‚¤ãƒ—ï¼‰ã‚’ææ¡ˆã™ã‚‹ã“ã¨ã§ã™ã€‚",
            # "ä»‹å…¥ã—ãªãã¦ã‚‚è‡ªå‹•çš„ã«å®‰å®šçŠ¶æ…‹ã«ãªã‚Šãã†ãªå ´åˆã«ã¯ä»‹å…¥ã—ãªã„é¸æŠã‚’ã™ã‚‹ã“ã¨ã‚‚æœ‰åŠ¹ã§ã™ã€‚"
            "â€»ãƒ­ãƒœãƒƒãƒˆã®å®Ÿéš›ã®ç™ºè©±æ–‡ã¯åˆ¥LLMãŒç”Ÿæˆã—ã¾ã™ã€‚ã‚ãªãŸã¯ä»‹å…¥æ–¹æ³•ã ã‘ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚",
            "",
            "åˆ¶ç´„:",
            "- å‡ºåŠ›ã¯ JSON ã®ã¿ã€‚èª¬æ˜ã‚„è£…é£¾ã¯ç¦æ­¢ã€‚",
            "- intervene_now ã¯ true|falseï¼ˆä»Šã™ãä»‹å…¥ã™ã¹ãã‹ï¼‰ã€‚",
            "- edge_to_change ã¯ \"AB\" | \"BC\" | \"CA\" ã®ã„ãšã‚Œã‹ã€‚",
            "- strategy ã¯ " + " | ".join(_PLANNER_STRATEGIES) + " ã‹ã‚‰é¸æŠã€‚",
            "  - reframe: å¦å®šçš„çŠ¶æ³ã‚’è‚¯å®šçš„è¦–ç‚¹ã‹ã‚‰å†è§£é‡ˆã€èªçŸ¥ã‚’è»¢æ›",
            "  - validate: å¯¾è±¡è€…ã®æ„Ÿæƒ…ãƒ»æ„è¦‹ã‚’æ‰¿èªã—ã€å¿ƒç†çš„å®‰å…¨æ€§ã‚’æ§‹ç¯‰",
            "  - bridge: å¯¾ç«‹ã™ã‚‹è€…ã®å…±é€šç‚¹ãƒ»ç›®æ¨™ã‚’æ˜ç¤ºã—ã€å”åŠ›é–¢ä¿‚ã‚’æ§‹ç¯‰",
            "  â€»ã©ã®æˆ¦ç•¥ã‚’ã„ã¤ä½¿ã†ã‹ã¯ä¼šè©±æ–‡è„ˆã‚„é–¢ä¿‚ã‚¹ã‚³ã‚¢ã‹ã‚‰åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚",
            "- target_speaker ã¯ A | B | C ã®ã„ãšã‚Œã‹ã€‚ï¼ˆãã®ä»‹å…¥ã‚’èª°ã«å‘ã‘ã‚‹ã‹ï¼‰",
            "- intervene_now=true ã®å ´åˆã¯ edge_to_change / strategy / target_speaker ã‚’å¿…ãšæŒ‡å®šã€‚",
            "- intervene_now=false ã®å ´åˆã¯ä½•ã‚‚æŒ‡å®šã—ãªã„ã€‚",
            "",
            "å±¥æ­´:",
            *history_lines,
            "",
            "ç¾åœ¨ã®é–¢ä¿‚ã‚¹ã‚³ã‚¢ï¼ˆ-1..1ï¼‰: " + ", ".join(f"w_{edge}={value:+.2f}" for edge, value in scores.items()),
            # f"æ¨å®šå®‰å®šãƒ‘ã‚¿ãƒ¼ãƒ³: {stable_sign}",
            # f"æ¨å¥¨ãƒªãƒ³ã‚¯å€™è£œ: {target_edge}",
            # f"è©•ä¾¡çª“ H_eval: {eval_horizon}",
            # f"æ™‚é–“ãƒšãƒŠãƒ«ãƒ†ã‚£ Î±: {time_penalty:.3f}",
            # f"ä»‹å…¥ã‚³ã‚¹ãƒˆ c_act: {intervention_cost:.3f}",
            "",
            "å‡ºåŠ›ä¾‹ï¼ˆJSONã®ã¿ï¼‰:",
            '{"intervene_now": true, "edge_to_change": "AB", "strategy": "reframe", "target_speaker": "A"}',
            '{"intervene_now": false}',
        ]
        return "\n".join(prompt_lines)

    def relation_snapshot(self) -> Dict[str, Any]:
        # ç¾åœ¨ã®é–¢ä¿‚ã‚¹ã‚³ã‚¢ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’è¿”ã™
        participants = self._participants(self.logs)
        scores = self.scorer.get_scores(self.logs, participants, return_trace=False, update_state=False)
        metrics = analyze_relations_from_scores(scores, include_nodes=participants)
        u_flip, stable_sign, distances = self._compute_u_flip(self._edge_weights_from_scores(scores))
        return {
            "scores": scores,
            "metrics": metrics,
            "u_flip": u_flip,
            "stable_sign": stable_sign,
            "distances": distances,
        }

    def _parse_plan(self, action: str) -> Tuple[Optional[Dict[str, Any]], Optional[str], Optional[str]]:
        # å…ˆé ­ãƒ»æœ«å°¾ã®å¼•ç”¨ç¬¦ã‚„æ‹¬å¼§ã‚’å‰Šã‚‹
        if action is None:
            return None, "empty_action", None
        text = action.strip()
        if not text:
            return None, "empty_action", None

        try:
            payload = json.loads(text)
        except json.JSONDecodeError:
            return None, "not_json", text

        if not isinstance(payload, dict):
            return None, "plan_not_object", text

        plan: Dict[str, Any] = {}
        intervene_now = _coerce_bool(payload.get("intervene_now"))
        if intervene_now is None:
            intervene_now = bool(payload.get("intervene_now"))
        plan["intervene_now"] = bool(intervene_now)

        edge_raw = str(payload.get("edge_to_change", "AB")).upper().strip()
        if edge_raw not in {"AB", "BC", "CA"}:
            edge_raw = "AB"
        plan["edge_to_change"] = edge_raw

        strategy_raw = str(payload.get("strategy", "reframe")).strip()
        if strategy_raw not in _PLANNER_STRATEGIES:
            strategy_raw = "reframe"
        plan["strategy"] = strategy_raw

        target_raw = str(payload.get("target_speaker", "A")).upper().strip()
        if target_raw not in {"A", "B", "C"}:
            target_raw = "A"
        plan["target_speaker"] = target_raw

        recheck_raw = _coerce_int(payload.get("recheck_after_turns", self.evaluation_horizon))
        if recheck_raw is None or recheck_raw <= 0:
            recheck_raw = self.evaluation_horizon
        plan["recheck_after_turns"] = max(self.evaluation_horizon, recheck_raw)

        return plan, None, None

    def _render_intervention(self, plan: Dict[str, Any], *, simulate: bool) -> Dict[str, Any]:
        # ä»‹å…¥ç™ºè©±ã‚’ç”Ÿæˆã™ã‚‹
        labels = self._human_labels()
        target_char = plan.get("target_speaker", "A")
        edge_to_change = plan.get("edge_to_change", "AB")
        partner_char = next((ch for ch in edge_to_change if ch != target_char), "B")

        try:
            target_idx = "ABC".index(target_char)
            target_name = labels[target_idx]
        except ValueError:
            target_name = target_char
        try:
            partner_idx = "ABC".index(partner_char)
            partner_name = labels[partner_idx]
        except ValueError:
            partner_name = partner_char

        strategy = plan.get("strategy")

        # max_historyå€‹ã®äººé–“ç™ºè©± + ãã®é–“ã®ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’å–å¾—
        filtered_logs = filter_logs_by_human_count(self.logs, self.max_history)
        history_lines = [
            f"[{entry.get('speaker', '?')}] {entry.get('utterance', '').strip()}" for entry in filtered_logs
        ]
        history_text = "\n".join(history_lines) if history_lines else "(å±¥æ­´ãªã—)"

        directive_map = {
            "reframe": f"ç›´å‰ã®{target_name}ã•ã‚“ã‚„ä»–ã®å‚åŠ è€…ã®ç™ºè¨€ã‹ã‚‰ã€å¦å®šçš„ãªå´é¢ã§ã¯ãªãè‚¯å®šçš„ãªæ„å›³ã‚„å»ºè¨­çš„ãªè¦–ç‚¹ã‚’è¦‹å‡ºã—ã€ãã‚Œã‚’ä¸€æ–‡ã§ä¼ãˆã¦ãã ã•ã„ã€‚å¯¾ç«‹ã‚’ã€è¦–ç‚¹ã®é•ã„ã€ã¨ã—ã¦ä¾¡å€¤ã‚ã‚‹ã‚‚ã®ã«å†å®šç¾©ã—ã¦ãã ã•ã„ã€‚",
            "validate": f"{target_name}ã•ã‚“ã®æ„Ÿæƒ…ã‚„æ„è¦‹ã‚’æ˜ç¢ºã«æ‰¿èªãƒ»å…±æ„Ÿã—ã€ãã®ä¾¡å€¤ã‚’èªã‚ã¦ãã ã•ã„ã€‚å¿ƒç†çš„å®‰å…¨æ€§ã‚’ä½œã‚Šã€å»ºè¨­çš„ãªå¯¾è©±ã‚’å¯èƒ½ã«ã™ã‚‹ä¸€æ–‡ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚",
            "bridge": f"{target_name}ã•ã‚“ã¨{partner_name}ã•ã‚“ã®é–“ã«å…±é€šç‚¹ãƒ»å…±é€šã®ç›®æ¨™ãƒ»ç›¸äº’ä¾å­˜æ€§ã‚’è¦‹å‡ºã—ã¦ãã ã•ã„ã€‚å¯¾ç«‹ã§ã¯ãªãå”åŠ›é–¢ä¿‚ã¨ã—ã¦æ‰ãˆç›´ã›ã‚‹è¦–ç‚¹ã‚’ä¸€æ–‡ã§æç¤ºã—ã¦ãã ã•ã„ã€‚",
        }
        directive = directive_map.get(strategy, directive_map["reframe"])

        fallback_text = self._fallback_intervention_text(target_name, partner_name, strategy)
        if simulate:
            return {"speaker": "ãƒ­ãƒœãƒƒãƒˆ", "utterance": fallback_text}

        cfg = get_config()
        llm_cfg = getattr(cfg, "llm", None)
        client, deployment = get_azure_chat_completion_client(llm_cfg, model_type="robot")
        max_attempts = getattr(cfg.llm, "max_attempts", 5) or 5
        base_backoff = getattr(cfg.llm, "base_backoff", 0.5) or 0.5
        if client and deployment:
            user_payload = (
                "ã‚ãªãŸã¯é–¢ä¿‚æ€§ã‚’å®‰å®šã•ã›ã‚‹ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±ç”Ÿæˆå™¨ã§ã™ã€‚å‡ºåŠ›ã¯æ—¥æœ¬èªã§ä¸€æ–‡ã®ã¿ã€‚è©±è€…ãƒ©ãƒ™ãƒ«ã‚„æ‹¬å¼§ã¯ä½¿ã‚ãªã„ã€‚\n"
                "ä¼šè©±å±¥æ­´ã‚’å‚è€ƒã«ã—ãªãŒã‚‰ã€ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ç™ºè©±ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n"
                f"æŒ‡ç¤º: {directive}\n"
                f"ä¼šè©±å±¥æ­´:\n{history_text}\n\n"
                "å¿…é ˆæ¡ä»¶:\n"
                f"- {target_name} ã•ã‚“ã®åå‰ã‚’ä¸€åº¦ã ã‘å…¥ã‚Œã‚‹ã€‚\n"
                "- ç›´å‰ã®è©±é¡Œã‚’è‡ªç„¶ã«å¼•ãç¶™ãã€‚\n"
                "- ä¸€æ–‡ã®ã¿ (60å­—å‰å¾Œ)ã€‚\n"
            )
            messages = [
                {"role": "system", "content": "ã‚ãªãŸã¯æˆ¦ç•¥ã‚’å…ƒã«ã€ä»‹å…¥ãƒ­ãƒœãƒƒãƒˆã®ç™ºè©±å†…å®¹ã‚’ä½œæˆã—ã¾ã™ã€‚å¸¸ã«æ—¥æœ¬èªã®ä¸€æ–‡ã ã‘ã‚’è¿”ã—ã¾ã™ã€‚"},
                {"role": "user", "content": user_payload},
            ]
            for attempt in range(1, max_attempts + 1):
                try:
                    # GPT-5ã®å ´åˆã¯reasoningãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    params = build_chat_completion_params(deployment, messages, cfg.llm)
                    res = client.chat.completions.create(**params)
                    if res and getattr(res, "choices", None):
                        choice = res.choices[0]
                        message = getattr(choice, "message", None)
                        if isinstance(message, dict):
                            txt = message.get("content", "")
                        else:
                            txt = getattr(message, "content", "")
                        txt = (txt or "").strip()
                        cleaned = _strip_quotes(txt or "")
                        if cleaned:
                            return {"speaker": "ãƒ­ãƒœãƒƒãƒˆ", "utterance": cleaned}
                except Exception as exc:
                    if getattr(_CFG.env, "debug", False):
                        print(f"[robot_utterance] attempt {attempt} failed:", exc)
                    if attempt < max_attempts:
                        time.sleep(base_backoff * (2 ** (attempt - 1)))
                    else:
                        if getattr(_CFG.env, "debug", False):
                            print("[robot_utterance] all attempts failed, falling back to local heuristic")
        return {"speaker": "ãƒ­ãƒœãƒƒãƒˆ", "utterance": fallback_text}

    def _fallback_intervention_text(self, target_name: str, partner_name: str, strategy: str) -> str:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ä»‹å…¥ç™ºè©±ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        templates = {
            "reframe": f"{target_name}ã•ã‚“ã€ãã®è¦–ç‚¹ã«ã¯å»ºè¨­çš„ãªæ„å›³ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚",
            "validate": f"{target_name}ã•ã‚“ã€ã‚ãªãŸã®æ„è¦‹ã¯ã¨ã¦ã‚‚å¤§åˆ‡ã§ã™ã€‚ã‚‚ã£ã¨èã‹ã›ã¦ãã ã•ã„ã€‚",
            "bridge": f"{target_name}ã•ã‚“ã€{partner_name}ã•ã‚“ã€ãŠäºŒäººã«ã¯å…±é€šã®ç›®æ¨™ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚",
        }
        return templates.get(strategy, templates["reframe"])

    def _bootstrap_humans(self, target_turns: int) -> None:
        # äººé–“å‚åŠ è€…ã®ç™ºè©±ã‚’è¿½åŠ ã—ã¦ä¼šè©±ã‚’é€²ã‚ã‚‹
        turns_added = 0
        safety_limit = max(1, self.max_steps * max(1, len(self.persona_pool)) * 3)
        while turns_added < target_turns and turns_added < safety_limit:
            # 1äººé–“ç™ºè©±ãšã¤ç”Ÿæˆ
            replies = human_reply(self.logs, self.persona_pool, topic=self.current_topic, topic_trigger=self.current_topic_trigger, num_speakers=1)
            if not replies:
                break
            self.logs.extend(replies)
            participants = self._participants(self.logs)
            if participants:
                # 3ç™ºè©±å¾Œã‹ã‚‰é–¢ä¿‚æ€§æ¨å®šã‚’æ›´æ–°
                human_utterance_count = sum(1 for log in self.logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')
                if human_utterance_count >= self.start_relation_check_after_utterances:
                    # é–¢ä¿‚æ€§LLMã«ã¯ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’é™¤å¤–ã—ã¦æ¸¡ã™
                    filtered_logs = filter_logs_by_human_count(self.logs, self.max_history_relation, exclude_robot=True)
                    self.scorer.get_scores(filtered_logs, participants, return_trace=False, update_state=True)
            turns_added += 1

    async def _bootstrap_humans_async(
        self,
        logs: List[Dict[str, Any]],
        target_turns: int,
        scorer: RelationScorer,
    ) -> List[Dict[str, Any]]:
        """
        äººé–“å‚åŠ è€…ã®ç™ºè©±ã‚’éåŒæœŸã§è¿½åŠ ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰

        Args:
            logs: ä¼šè©±ãƒ­ã‚°ï¼ˆå¤‰æ›´ã•ã‚Œãªã„ã€ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ç”¨ï¼‰
            target_turns: ç”Ÿæˆã™ã‚‹äººé–“ç™ºè©±ã®å›æ•°
            scorer: é–¢ä¿‚æ€§ã‚¹ã‚³ã‚¢ãƒ©ãƒ¼

        Returns:
            æ›´æ–°ã•ã‚ŒãŸä¼šè©±ãƒ­ã‚°
        """
        logs_copy = copy.deepcopy(logs)
        turns_added = 0
        safety_limit = max(1, self.max_steps * max(1, len(self.persona_pool)))

        while turns_added < target_turns and turns_added < safety_limit:
            # human_replyã¯åŒæœŸé–¢æ•°ãªã®ã§ã€asyncio.to_thread()ã§ãƒ©ãƒƒãƒ—
            # 1äººé–“ç™ºè©±ãšã¤ç”Ÿæˆ
            replies = await asyncio.to_thread(
                human_reply,
                logs_copy,
                self.persona_pool,
                topic=self.current_topic,
                topic_trigger=self.current_topic_trigger,
                num_speakers=1
            )
            if not replies:
                break
            logs_copy.extend(replies)
            participants = self._participants(logs_copy)
            if participants:
                human_utterance_count = sum(1 for log in logs_copy if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')
                if human_utterance_count >= self.start_relation_check_after_utterances:
                    filtered_logs = filter_logs_by_human_count(logs_copy, self.max_history_relation, exclude_robot=True)
                    # get_scoresã‚‚åŒæœŸé–¢æ•°ãªã®ã§ã€asyncio.to_thread()ã§ãƒ©ãƒƒãƒ—
                    await asyncio.to_thread(
                        scorer.get_scores,
                        filtered_logs,
                        participants,
                        return_trace=False,
                        update_state=True
                    )
            turns_added += len([r for r in replies if r.get("speaker") != "ãƒ­ãƒœãƒƒãƒˆ"])

        return logs_copy

    def _ensure_unstable_seed(self) -> None:
        # åˆæœŸçŠ¶æ…‹ãŒä¸å®‰å®šã«ãªã‚‹ã¾ã§äººé–“ç™ºè©±ã‚’è¿½åŠ ã™ã‚‹
        attempts = 0
        while self._is_balanced() and attempts < self.max_steps:
            self._bootstrap_humans(self.evaluation_horizon)
            attempts += 1

    def _participants(self, logs: List[Dict[str, Any]]) -> List[str]:
        # ç™ºè©±è€…ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ï¼ˆãƒ­ãƒœãƒƒãƒˆé™¤å¤–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
        names = {entry.get("speaker") for entry in logs if entry.get("speaker") and entry.get("speaker") != "ãƒ­ãƒœãƒƒãƒˆ"}
        return sorted(names)

    def _human_labels(self) -> List[str]:
        # äººé–“å‚åŠ è€…ã®ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹
        labels = [name for name in self.persona_pool if name]
        while len(labels) < 3:
            labels.append(chr(ord("A") + len(labels)))
        return labels[:3]

    def _edge_weights(self, logs: List[Dict[str, Any]]) -> Dict[str, float]:
        # ã‚¨ãƒƒã‚¸ã®é‡ã¿ã‚’è¨ˆç®—ã™ã‚‹
        participants = self._participants(logs)
        scores = self.scorer.get_scores(logs, participants, return_trace=False, update_state=False)
        return self._edge_weights_from_scores(scores)

    def _edge_weights_from_scores(self, scores: Dict[Tuple[str, str], float]) -> Dict[str, float]:
        # ã‚¹ã‚³ã‚¢ã‹ã‚‰ã‚¨ãƒƒã‚¸ã®é‡ã¿ã‚’è¨ˆç®—ã™ã‚‹
        labels = self._human_labels()
        key_map = {
            "AB": tuple(sorted((labels[0], labels[1]))),
            "BC": tuple(sorted((labels[1], labels[2]))),
            "CA": tuple(sorted((labels[2], labels[0]))),
        }
        return {edge: float(scores.get(pair, 0.0)) for edge, pair in key_map.items()}

    def _relation_state(self, logs: List[Dict[str, Any]], *, update_state: bool) -> Tuple[Dict[Tuple[str, str], float], List[str]]:
        # é–¢ä¿‚ã‚¹ã‚³ã‚¢ã®çŠ¶æ…‹ã‚’å–å¾—ã™ã‚‹
        participants = self._participants(logs)
        scores, trace = self.scorer.get_scores(logs, participants, return_trace=True, update_state=update_state)
        return scores, trace

    def _metrics_state(self, scores: Dict[Tuple[str, str], float], participants: List[str]) -> Tuple[Dict[str, Any], List[str]]:
        # é–¢ä¿‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®çŠ¶æ…‹ã‚’å–å¾—ã™ã‚‹
        metrics, trace = analyze_relations_from_scores(scores, include_nodes=participants, verbose=True, return_trace=True)
        return metrics, trace

    def _compute_u_flip(self, weights: Dict[str, float]) -> Tuple[float, str, Dict[str, float]]:
        # ä¸å®‰å®šåº¦ u_flip ã¨æœ€é©å®‰å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—ã™ã‚‹
        best_total = math.inf
        best_sign = "+++"
        best_distances: Dict[str, float] = {edge: 0.0 for edge in weights}
        for sign, mapping in _STABLE_SIGN_PATTERNS.items():
            total = 0.0
            distances: Dict[str, float] = {}
            for edge, direction in mapping.items():
                weight = weights.get(edge, 0.0)
                distance = max(0.0, -direction * weight)
                distances[edge] = distance
                total += distance
            if total < best_total:
                best_total = total
                best_sign = sign
                best_distances = distances
        if not math.isfinite(best_total):
            best_total = 0.0
        return best_total, best_sign, best_distances

    def _compute_u_flip_from_scores(self, scores: Dict[Tuple[str, str], float]) -> float:
        weights = self._edge_weights_from_scores(scores)
        u_flip, _, _ = self._compute_u_flip(weights)
        return u_flip

    def _simulate_forward(
        self,
        snapshot: List[Dict[str, Any]],
        human_turns: int,
        plan: Optional[Dict[str, Any]],
    ) -> float:
        """
        å°†æ¥ã®äººé–“ç™ºè©±ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¦ä¸å®‰å®šåº¦ u_flip ã‚’è¨ˆç®—ã™ã‚‹
        
        é…å»¶å ±é…¬ã®è¨­è¨ˆ:
        - ãƒ­ãƒœãƒƒãƒˆãŒä»‹å…¥ã™ã‚‹å ´åˆ: ãƒ­ãƒœãƒƒãƒˆç™ºè©± + human_turnså›ã®äººé–“ç™ºè©±å¾Œã®é–¢ä¿‚æ€§
        - ãƒ­ãƒœãƒƒãƒˆãŒä»‹å…¥ã—ãªã„å ´åˆ: human_turnså›ã®äººé–“ç™ºè©±å¾Œã®é–¢ä¿‚æ€§
        
        Args:
            snapshot: ç¾åœ¨ã®ä¼šè©±ãƒ­ã‚°ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
            human_turns: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹äººé–“ç™ºè©±ã®å›æ•°ï¼ˆevaluation_horizonï¼‰
            plan: ãƒ­ãƒœãƒƒãƒˆã®ä»‹å…¥ãƒ—ãƒ©ãƒ³ï¼ˆNoneã®å ´åˆã¯ä»‹å…¥ãªã—ï¼‰
        
        Returns:
            u_flip: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆå¾Œã®ä¸å®‰å®šåº¦
        """
        logs = copy.deepcopy(snapshot)
        scorer = RelationScorer(**self._scorer_kwargs)

        # ãƒ­ãƒœãƒƒãƒˆãŒä»‹å…¥ã™ã‚‹å ´åˆã€ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’è¿½åŠ 
        if plan and plan.get("intervene_now"):
            logs.append(self._render_intervention(plan, simulate=True))

        # human_turnså›ã®äººé–“ç™ºè©±ã‚’è¿½åŠ 
        turns_added = 0
        safety_limit = max(1, self.max_steps * max(1, len(self.persona_pool)))
        while turns_added < human_turns and turns_added < safety_limit:
            # 1äººé–“ç™ºè©±ãšã¤ç”Ÿæˆ
            replies = human_reply(logs, self.persona_pool, topic=self.current_topic, topic_trigger=self.current_topic_trigger, num_speakers=1)
            if not replies:
                break
            logs.extend(replies)
            participants = self._participants(logs)
            if participants:
                # é–¢ä¿‚æ€§æ¨å®šã‚’3ç™ºè©±å¾Œã‹ã‚‰è¡Œã†
                human_utterance_count = sum(1 for log in logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')
                if human_utterance_count >= self.start_relation_check_after_utterances:
                    # é–¢ä¿‚æ€§LLMã«ã¯ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’é™¤å¤–ã—ã¦æ¸¡ã™
                    filtered_logs = filter_logs_by_human_count(logs, self.max_history_relation, exclude_robot=True)
                    scorer.get_scores(filtered_logs, participants, return_trace=False, update_state=True)
            turns_added += len([r for r in replies if r.get("speaker") != "ãƒ­ãƒœãƒƒãƒˆ"])

        # human_turnså›å¾Œã®é–¢ä¿‚æ€§ã‚’è¨ˆç®—
        participants = self._participants(logs)
        human_utterance_count = sum(1 for log in logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')
        if human_utterance_count >= self.start_relation_check_after_utterances and participants:
            # é–¢ä¿‚æ€§LLMã«ã¯ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’é™¤å¤–ã—ã¦æ¸¡ã™
            filtered_logs = filter_logs_by_human_count(logs, self.max_history_relation, exclude_robot=True)
            scores = scorer.get_scores(filtered_logs, participants, return_trace=False, update_state=False)
            weights = self._edge_weights_from_scores(scores)
            u_flip, _, _ = self._compute_u_flip(weights)
        else:
            # 3ç™ºè©±æœªæº€ã®å ´åˆã¯ä¸å®‰å®šåº¦ã‚’æœ€å¤§å€¤ï¼ˆ1.0ï¼‰ã«è¨­å®š
            u_flip = 1.0

        return u_flip

    async def _simulate_forward_async(
        self,
        snapshot: List[Dict[str, Any]],
        human_turns: int,
        plan: Optional[Dict[str, Any]],
    ) -> Tuple[float, List[Dict[str, Any]]]:
        """
        å°†æ¥ã®äººé–“ç™ºè©±ã‚’éåŒæœŸã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¦ä¸å®‰å®šåº¦ u_flip ã‚’è¨ˆç®—ã™ã‚‹

        Args:
            snapshot: ç¾åœ¨ã®ä¼šè©±ãƒ­ã‚°ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
            human_turns: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹äººé–“ç™ºè©±ã®å›æ•°ï¼ˆevaluation_horizonï¼‰
            plan: ãƒ­ãƒœãƒƒãƒˆã®ä»‹å…¥ãƒ—ãƒ©ãƒ³ï¼ˆNoneã®å ´åˆã¯ä»‹å…¥ãªã—ï¼‰

        Returns:
            (u_flip, logs): ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆå¾Œã®ä¸å®‰å®šåº¦ã¨æ›´æ–°ã•ã‚ŒãŸä¼šè©±ãƒ­ã‚°
        """
        logs = copy.deepcopy(snapshot)
        scorer = RelationScorer(**self._scorer_kwargs)

        # ãƒ­ãƒœãƒƒãƒˆãŒä»‹å…¥ã™ã‚‹å ´åˆã€ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’è¿½åŠ 
        if plan and plan.get("intervene_now"):
            logs.append(self._render_intervention(plan, simulate=True))

        # human_turnså›ã®äººé–“ç™ºè©±ã‚’éåŒæœŸã§è¿½åŠ 
        logs = await self._bootstrap_humans_async(logs, human_turns, scorer)

        # human_turnså›å¾Œã®é–¢ä¿‚æ€§ã‚’è¨ˆç®—
        participants = self._participants(logs)
        human_utterance_count = sum(1 for log in logs if log.get('speaker') != 'ãƒ­ãƒœãƒƒãƒˆ')
        if human_utterance_count >= self.start_relation_check_after_utterances and participants:
            # é–¢ä¿‚æ€§LLMã«ã¯ãƒ­ãƒœãƒƒãƒˆç™ºè©±ã‚’é™¤å¤–ã—ã¦æ¸¡ã™
            filtered_logs = filter_logs_by_human_count(logs, self.max_history_relation, exclude_robot=True)
            # get_scoresã‚’éåŒæœŸã§å®Ÿè¡Œ
            scores = await asyncio.to_thread(
                scorer.get_scores,
                filtered_logs,
                participants,
                return_trace=False,
                update_state=False
            )
            weights = self._edge_weights_from_scores(scores)
            u_flip, _, _ = self._compute_u_flip(weights)
        else:
            # 3ç™ºè©±æœªæº€ã®å ´åˆã¯ä¸å®‰å®šåº¦ã‚’æœ€å¤§å€¤ï¼ˆ1.0ï¼‰ã«è¨­å®š
            u_flip = 1.0

        return u_flip, logs

    def _is_balanced(self) -> bool:
        # ç¾åœ¨ã®ä¼šè©±ãƒ­ã‚°ã«åŸºã¥ãã€é–¢ä¿‚ãŒå®‰å®šçŠ¶æ…‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        snapshot = self.relation_snapshot()
        triangles = snapshot.get("triangles", {})
        if not triangles:
            return False
        return all(status == "S" for status in triangles.values())

    # ------------------------------------------------------------------
    # Validation
    # ------------------------------------------------------------------
    def _validate_plan(self, plan: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
        # ä»‹å…¥ãƒ—ãƒ©ãƒ³ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
        if not isinstance(plan, dict):
            return False, "plan_not_dict"
        if "intervene_now" not in plan:
            return False, "missing_intervene_now"
        try:
            recheck = int(plan.get("recheck_after_turns", self.evaluation_horizon))
        except Exception:
            return False, "invalid_recheck"
        if recheck <= 0:
            return False, "recheck_non_positive"
        if recheck < self.evaluation_horizon:
            return False, "recheck_too_short"

        intervene_now = bool(plan.get("intervene_now"))
        if intervene_now:
            edge = plan.get("edge_to_change")
            strategy = plan.get("strategy")
            target = plan.get("target_speaker")
            if edge not in _TARGET_EDGES:
                return False, "invalid_edge"
            if strategy not in _ALLOWED_STRATEGIES:
                return False, "invalid_strategy"
            if target not in _TARGET_SPEAKERS:
                return False, "invalid_target"
        return True, None

    # ------------------------------------------------------------------
    # Diagnostics
    # ------------------------------------------------------------------
    def relation_snapshot(self) -> Dict[str, Any]:
        # ç¾åœ¨ã®ä¼šè©±ãƒ­ã‚°ã«åŸºã¥ãã€é–¢ä¿‚ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—
        participants = self._participants(self.logs)
        scores = self.scorer.get_scores(self.logs, participants, return_trace=False, update_state=False)
        relations = analyze_relations_from_scores(scores, include_nodes=participants, verbose=self.debug)
        triangles = {}
        for tri in relations.get("triangles", []):
            nodes = tuple(sorted(tri.get("nodes", [])))
            if any(node == "ãƒ­ãƒœãƒƒãƒˆ" for node in nodes):
                continue
            struct = tri.get("struct")
            if struct in BALANCED:
                status = "S"
            elif struct in UNBALANCED:
                status = "U"
            else:
                status = "?"
            triangles[nodes] = status
        return {
            "participants": [p for p in participants if p != "ãƒ­ãƒœãƒƒãƒˆ"],
            "triangles": triangles,
            "scores": scores,
        }
